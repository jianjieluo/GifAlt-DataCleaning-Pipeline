{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import functools\n",
    "\n",
    "import spacy\n",
    "import en_core_web_lg\n",
    "\n",
    "from rules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual(doc, _type):\n",
    "    if _type == 'entity':\n",
    "        for ent in doc.ents: \n",
    "            print(ent.label_, '\\t', ent.text)\n",
    "    if _type == 'noun_chunks':\n",
    "        for chunk in doc.noun_chunks:\n",
    "            print(chunk.root.text, '\\t', chunk.text)\n",
    "\n",
    "def precook(sent, vocab):\n",
    "    # similar to basic_cleaner.py\n",
    "    sent = sent.lower()\n",
    "    tokens = nltk.word_tokenize(sent)\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    sent = ' '.join(tokens)\n",
    "    sent = sent.strip()\n",
    "    return sent\n",
    "\n",
    "def read_txt(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data = [l.strip() for l in f]\n",
    "    return data\n",
    "\n",
    "wiki_vocab = read_txt('cache/vocab_clean.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\b(january|february|march|april|june|july|august|september|october|november|december|in january|in february|in march|in april|in may|in june|in july|in august|in september|in october|in november|in december)\\\\b'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _build_useless_pattern():\n",
    "        # Month\n",
    "        month = [x.lower() for x in MONTH]\n",
    "        in_month = ['in ' + x for x in month]\n",
    "        month += in_month\n",
    "        month.remove('may')\n",
    "        # Maybe Others...\n",
    "\n",
    "        # Combined\n",
    "        useless = month\n",
    "        r = r'\\b(%s)\\b' % '|'.join(useless)\n",
    "        return r\n",
    "    \n",
    "useless_pattern = _build_useless_pattern()\n",
    "display(useless_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load language model...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# __init__\n",
    "print(\"Load language model...\")\n",
    "nlp = en_core_web_lg.load()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict = {\n",
    "            'IN': 'IN',\n",
    "            'DT': 'DT', 'PDT': 'DT',\n",
    "            'JJ': 'JJ', 'JJR': 'JJ', 'JJS': 'JJ', \n",
    "            'NN': 'NN', 'NNS': 'NN', 'NNP': 'NN', 'NNPS': 'NN',\n",
    "        }\n",
    "\n",
    "grammar1 = r\"\"\"\n",
    "            CHUNK: {<IN>+<DT|PRP\\$>?<JJ>*<NN>+}\n",
    "        \"\"\"\n",
    "chunk_parser1 = nltk.RegexpParser(grammar1)\n",
    "\n",
    "grammar2 = r\"\"\"\n",
    "            NP: {<NN><NN>+}\n",
    "        \"\"\"\n",
    "chunk_parser2 = nltk.RegexpParser(grammar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'harrison ford and calista flockhart attend the premiere of hollywood homicide at the american film festival september in deauville france'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt_texts = [\n",
    "    \"Harrison Ford and Calista Flockhart attend the premiere of Hollywood Homicide at the 29th American Film Festival September 5, 2003 in Deauville, France.\",\n",
    "    \"Side view of a British Airways Airbus A319 aircraft on approach to land with landing gear down\",\n",
    "    \"Two sculptures by artist Duncan McKellar adorn trees outside the derelict Norwich Union offices in Bristol, UK\",\n",
    "    \"A Pakistani worker helps to clear the debris from the Taj Mahal Hotel November 7, 2005 in Balakot, Pakistan.\",\n",
    "    \"Musician Justin Timberlake performs at the 2017 Pilgrimage Music & Cultural Festival on September 23, 2017 in Franklin, Tennessee.\"\n",
    "]\n",
    "alt_text = precook(alt_texts[0].lower(), wiki_vocab)\n",
    "alt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'harrison ford and calista flockhart attend the premiere of hollywood homicide at the american film festival in deauville france'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove Date month\n",
    "text = alt_text\n",
    "text = re.sub(useless_pattern, \"\", text)\n",
    "text = ' '.join(text.split())\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'person_li'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['harrison ford', 'calista flockhart']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'dt_li'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'the'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'harrison ford and calista flockhart attend the premiere of homicide at the festival in france'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# noun_chunker_simplify\n",
    "doc = nlp(text)\n",
    "\n",
    "person_li = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "dt_li = set([token.text for token in doc if token.tag_ in ['DT', 'PDT']])\n",
    "\n",
    "display('person_li', person_li)\n",
    "display('dt_li', dt_li)\n",
    "\n",
    "def contain_person(chunk):\n",
    "    for p in person_li:\n",
    "        if p in chunk:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "li = []\n",
    "for chunk in doc.noun_chunks:\n",
    "    src = chunk.text\n",
    "\n",
    "    if contain_person(src):\n",
    "        continue\n",
    "\n",
    "    splits = src.split()\n",
    "    dst = ' '.join([splits[0], chunk.root.text]) \\\n",
    "          if splits[0] in dt_li \\\n",
    "          else chunk.root.text\n",
    "    li.append((src, dst))\n",
    "\n",
    "for item in li:\n",
    "    text = text.replace(item[0], item[1])\n",
    "\n",
    "text = ' '.join(text.split())\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON \t harrison ford\n",
      "PERSON \t calista flockhart\n",
      "GPE \t france\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'PERSON and PERSON attend the premiere of homicide at the festival in GPE'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# name_entity_hyper\n",
    "doc = nlp(text)\n",
    "hyper_classes = REPLACE_CLASSES + DROP_CLASSES\n",
    "newString = text\n",
    "visual(doc, 'entity')\n",
    "for e in reversed(doc.ents):\n",
    "    if e.label_ in hyper_classes:\n",
    "        start = e.start_char\n",
    "        end = start + len(e.text)\n",
    "        newString = newString[:start] + e.label_ + newString[end:]\n",
    "text = newString\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people attend the premiere of homicide at the festival in GPE\n"
     ]
    }
   ],
   "source": [
    "def _multi2single_head(text):\n",
    "    \"\"\"\n",
    "    hard code implement\n",
    "    \"\"\"\n",
    "    text = text.replace('PERSON and PERSON', 'people')\n",
    "    text = text.replace('PERSON, PERSON and PERSON', 'people')\n",
    "    return text\n",
    "\n",
    "text = _multi2single_head(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag(text):\n",
    "    doc = nlp(text)\n",
    "    pos_tags = [\n",
    "        (token.text, pos_dict[token.tag_] if token.tag_ in pos_dict else token.tag_) \\\n",
    "        for token in doc\n",
    "    ]\n",
    "    return pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  people/NN\n",
      "  attend/VBP\n",
      "  the/DT\n",
      "  premiere/NN\n",
      "  (CHUNK of/IN homicide/NN)\n",
      "  (CHUNK at/IN the/DT festival/NN)\n",
      "  (CHUNK in/IN GPE/NN))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'people attend the premiere of homicide at the festival'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove_pos_tag_patterns\n",
    "text = pos_tag(text)\n",
    "result = chunk_parser1.parse(text)\n",
    "print(result)\n",
    "terms = [[e[0]] if isinstance(e, tuple) else [w for w,t in e] for e in result]\n",
    "res = [x for x in terms if len(set(x).intersection(set(DROP_CLASSES))) == 0]\n",
    "text = ' '.join(functools.reduce(lambda x,y: x+y,res))\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  people/NN\n",
      "  attend/VBP\n",
      "  the/DT\n",
      "  premiere/NN\n",
      "  of/IN\n",
      "  homicide/NN\n",
      "  at/IN\n",
      "  the/DT\n",
      "  festival/NN)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'people attend the premiere of homicide at the festival'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = pos_tag(text)\n",
    "result = chunk_parser2.parse(text)\n",
    "print(result)\n",
    "terms = [e[0] if isinstance(e, tuple) else [w for w,t in e][-1] for e in result]\n",
    "text = ' '.join(terms)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON \t harrison ford\n",
      "PERSON \t calista flockhart\n",
      "GPE \t hollywood\n",
      "EVENT \t the american film festival september\n",
      "GPE \t deauville france\n"
     ]
    }
   ],
   "source": [
    "# Test nlp.disable_pipes\n",
    "# https://spacy.io/usage/processing-pipelines#disabling\n",
    "\n",
    "with nlp.disable_pipes(\"tagger\", \"parser\"):\n",
    "    doc = nlp(\"harrison ford and calista flockhart attend the premiere of hollywood homicide at the american film festival september in deauville france\")\n",
    "\n",
    "# visual(doc, 'noun_chunks')\n",
    "visual(doc, 'entity')\n",
    "# [\n",
    "#     (token.text, pos_dict[token.tag_] if token.tag_ in pos_dict else token.tag_) \\\n",
    "#     for token in doc\n",
    "# ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nltk)",
   "language": "python",
   "name": "nltk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
